# [ICLR 2026] UniHM: Unified Dexterous Hand Manipulation with Vision Language Model

This is the offical code repo for **ICLR 2026** paper **UniHM: Unified Dexterous Hand Manipulation with Vision Language Model**

<div align="center">
    <img src="pipeline.png" height=500>
</div>


# Disclaimers
- Code Quality Level: Tired grad student, lots of hard code in my repo
- Training Enviroment: A800 80G GPUs
- Questions: please drop me an email, it is the fastest way to get feedback
- For Enviroment Set Up: I set the enviroment in my gpus by this way, may have more easy ways. But I believe you can set up the enviroment by my steps

# Plan

- [ ] Paper Released.
- [√] Code.
- [√] Pretrained Weights.
- [√] Dataset.
- [√] Weights of UniHM

Any Question, feel free to contact zhangzhh2024@shanghaitech.edu.cn

# DataSet
[DexYCB](https://dex-ycb.github.io/)

[OAKINK](https://github.com/oakink/OakInk)
# Pretrain Weights
[Qwen3-0.6b](https://www.modelscope.cn/models/Qwen/Qwen3-0.6B)

# Weights of UniHM
[Weights](https://pan.baidu.com/s/1oYO_a6FOKloeHNCHiYnxVA?pwd=gcyb)

# Acknowledgement
Thanks for the excellent work [OpenHOI](https://github.com/Zhenhao-Zhang/OpenHOI),[DexGYS](https://github.com/iSEE-Laboratory/Grasp-as-You-Say),[AffordDP](https://github.com/SshiJwu/AffordDP),[CLIPort](https://github.com/huangwl18/ReKep),[MotionGPT](https://github.com/OpenMotionLab/MotionGPT)