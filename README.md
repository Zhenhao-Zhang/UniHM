# [ICLR 2026] UniHM: Unified Dexterous Hand Manipulation with Vision Language Model

This is the offical code repo for **ICLR 2026** paper **UniHM: Unified Dexterous Hand Manipulation with Vision Language Model**

<div align="center">
    <img src="pipeline.png" height=500>
</div>


# Disclaimers
- Code Quality Level: Tired grad student, lots of hard code in my repo
- Training Enviroment: A800 80G GPUs
- Questions: please drop me an email, it is the fastest way to get feedback
- For Enviroment Set Up: I set the enviroment in my gpus by this way, may have more easy ways. But I believe you can set up the enviroment by my steps

# Plan

- [ ] Paper Released.
- [ ] Code.
- [ ] Pretrained Weights.
- [ ] Dataset.
- [ ] Quick Start
- [ ] Weights of UniHM

Any Question, feel free to contact zhangzhh2024@shanghaitech.edu.cn


# Acknowledgement
Thanks for the excellent work [OpenHOI](https://github.com/Zhenhao-Zhang/OpenHOI),[DexGYS](https://github.com/iSEE-Laboratory/Grasp-as-You-Say),[AffordDP](https://github.com/SshiJwu/AffordDP),[CLIPort](https://github.com/huangwl18/ReKep),[MotionGPT](https://github.com/OpenMotionLab/MotionGPT)